{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alice in Wonderland -- A New Chapter!\n",
    "In this notebook I'm using LSTM to generate new text based on Alice in Wonderland. Let's see how it goes!\n",
    "\n",
    "## Data Source\n",
    "The original text is from project [Gutenberg](https://www.gutenberg.org/files/11/11-0.txt).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load Dataset from File\n",
    "    \"\"\"\n",
    "    import os\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\", encoding='utf-8') as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "data_file = 'Alice_in_Wonderland.txt'\n",
    "text = load_data(data_file)\n",
    "# Ignore license text and truncate to suitable size\n",
    "text = text[710:120000]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 4604\n",
      "Number of scenes: 659\n",
      "Average number of sentences in each scene: 2.1077389984825494\n",
      "Number of lines: 2048\n",
      "Average number of words in each line: 10.72119140625\n",
      "\n",
      "The sentences 1000 to 1050:\n",
      "   ‘You are old, Father William,’ the young man said,\n",
      "    ‘And your hair has become very white;\n",
      "   And yet you incessantly stand on your head--\n",
      "    Do you think, at your age, it is right?’\n",
      "\n",
      "   ‘In my youth,’ Father William replied to his son,\n",
      "    ‘I feared it might injure the brain;\n",
      "   But, now that I’m perfectly sure I have none,\n",
      "    Why, I do it again and again.’\n",
      "\n",
      "   ‘You are old,’ said the youth, ‘as I mentioned before,\n",
      "    And have grown most uncommonly fat;\n",
      "   Yet you turned a back-somersault in at the door--\n",
      "    Pray, what is the reason of that?’\n",
      "\n",
      "   ‘In my youth,’ said the sage, as he shook his grey locks,\n",
      "    ‘I kept all my limbs very supple\n",
      "   By the use of this ointment--one shilling the box--\n",
      "    Allow me to sell you a couple?’\n",
      "\n",
      "   ‘You are old,’ said the youth, ‘and your jaws are too weak\n",
      "    For anything tougher than suet;\n",
      "   Yet you finished the goose, with the bones and the beak--\n",
      "    Pray how did you manage to do it?’\n",
      "\n",
      "   ‘In my youth,’ said his father, ‘I took to the law,\n",
      "    And argued each case with my wife;\n",
      "   And the muscular strength, which it gave to my jaw,\n",
      "    Has lasted the rest of my life.’\n",
      "\n",
      "   ‘You are old,’ said the youth, ‘one would hardly suppose\n",
      "    That your eye was as steady as ever;\n",
      "   Yet you balanced an eel on the end of your nose--\n",
      "    What made you so awfully clever?’\n",
      "\n",
      "   ‘I have answered three questions, and that is enough,’\n",
      "    Said his father; ‘don’t give yourself airs!\n",
      "   Do you think I can listen all day to such stuff?\n",
      "    Be off, or I’ll kick you down stairs!’\n",
      "\n",
      "\n",
      "‘That is not said right,’ said the Caterpillar.\n",
      "\n",
      "‘Not QUITE right, I’m afraid,’ said Alice, timidly; ‘some of the words\n",
      "have got altered.’\n",
      "\n",
      "‘It is wrong from beginning to end,’ said the Caterpillar decidedly, and\n",
      "there was silence for some minutes.\n",
      "\n",
      "The Caterpillar was the first to speak.\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (1000, 1050)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len(set(text.split()))))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions\n",
    "A couple of handy functions to help with running the model later:\n",
    "- Lookup Table (words to integers and back)\n",
    "- Tokenize Punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = set(text)\n",
    "    vocab_to_int = {w:i for i, w in enumerate(vocab, 1)}\n",
    "    int_to_vocab = {v:k for k, v in vocab_to_int.items()}\n",
    "    \n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Punctuation\n",
    "Punctuations like periods and exclamation marks complicate things for the neural network. This function will tokenize symbols like \"!\" into \"||Exclamation_Mark||\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    \n",
    "    punctuation_dict = { '.': '||Period||',\n",
    "    ',': '||comma||',\n",
    "    '--': '||dash||',\n",
    "    '(': '||left_parentheses||',\n",
    "    ')': '||right_parentheses||',\n",
    "    ';': '||semicolon||',\n",
    "    '?': '||question||',\n",
    "    '\\n': '||return||',\n",
    "    '\"': '||double_quote||',\n",
    "    '!': '||exclamation||'}\n",
    "    \n",
    "    return punctuation_dict\n",
    "\n",
    "\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(text, token_lookup, create_lookup_tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "A quick check point so we don't have to start from the beginning next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network\n",
    "To simplify the many tasks of building the NN, we'll implement the following functions:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.7.1\n",
      "No GPU found.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.3'), 'Please use TensorFlow version 1.3 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "The `get_inputs()` function creates the TF Placeholders we need.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    \n",
    "    Inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    Targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    LearningRate = tf.placeholder(tf.float32, name='learningrate')\n",
    "    \n",
    "    \n",
    "    return (Inputs, Targets, LearningRate)\n",
    "\n",
    "\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initial state)\n",
    "    \"\"\"\n",
    "    \n",
    "    lstm_layers = 1\n",
    "    \n",
    "    stacked_rnn = []\n",
    "    for i in range(lstm_layers):\n",
    "        stacked_rnn.append(tf.contrib.rnn.BasicLSTMCell(rnn_size))\n",
    "    \n",
    "    #lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(stacked_rnn)\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "\n",
    "    return (cell, initial_state)\n",
    "\n",
    "\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    \n",
    "    return embed\n",
    "\n",
    "\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the RNN\n",
    "We'll use the cells from `get_init_cell()` to create a dynamic RNN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    \n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "    \n",
    "    return (outputs, final_state)\n",
    "\n",
    "\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Neural Network\n",
    "Putting all the pieces together, using the functions above, and adding a fully connected layer with linear activation to compute logits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    \n",
    "    embed_inputs = get_embed(input_data, vocab_size, embed_dim)\n",
    "    outputs, final_state = build_rnn(cell, embed_inputs)\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, vocab_size, activation_fn=None)\n",
    "    \n",
    "    return (logits, final_state)\n",
    "\n",
    "\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches\n",
    "The following function will create batches of inputs and targets using `int_text`.  The batches are a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    \n",
    "    step = batch_size * seq_length\n",
    "    \n",
    "    num_of_batches = (len(int_text) - 1) // step\n",
    "    int_text = int_text[:(num_of_batches * step) + 1] \n",
    "    \n",
    "    # set the last target token to the same value as the first input token\n",
    "    int_text[len(int_text) - 1] = int_text[0]\n",
    "  \n",
    "    # find the inputs\n",
    "    input_seq = [int_text[i * seq_length : i * seq_length + seq_length] for i in range(0, num_of_batches * batch_size)]\n",
    "    \n",
    "    # find the targets, same as inputs but shifted by 1\n",
    "    int_text = int_text[1:] \n",
    "    target_seq = [int_text[i * seq_length : i * seq_length + seq_length] for i in range(0, num_of_batches * batch_size)]\n",
    "    \n",
    "    outputs = []\n",
    "    for n in range(num_of_batches):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "        \n",
    "        for j in range(batch_size):\n",
    "            inputs.append(input_seq[j * num_of_batches + n])\n",
    "            targets.append(target_seq[j * num_of_batches + n])\n",
    "        outputs.append([inputs, targets])\n",
    "        \n",
    "    outputs = np.array(outputs)    \n",
    "    return outputs\n",
    "    \n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "The following parameters can be tuned as needed:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 512\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 512\n",
    "# Sequence Length\n",
    "seq_length = 50\n",
    "# Learning Rate\n",
    "learning_rate = .01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Adam Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/2   train_loss = 8.012\n",
      "Epoch   5 Batch    0/2   train_loss = 5.272\n",
      "Epoch  10 Batch    0/2   train_loss = 4.458\n",
      "Epoch  15 Batch    0/2   train_loss = 3.729\n",
      "Epoch  20 Batch    0/2   train_loss = 3.068\n",
      "Epoch  25 Batch    0/2   train_loss = 2.520\n",
      "Epoch  30 Batch    0/2   train_loss = 2.061\n",
      "Epoch  35 Batch    0/2   train_loss = 1.664\n",
      "Epoch  40 Batch    0/2   train_loss = 1.325\n",
      "Epoch  45 Batch    0/2   train_loss = 1.030\n",
      "Epoch  50 Batch    0/2   train_loss = 0.801\n",
      "Epoch  55 Batch    0/2   train_loss = 0.620\n",
      "Epoch  60 Batch    0/2   train_loss = 0.480\n",
      "Epoch  65 Batch    0/2   train_loss = 0.361\n",
      "Epoch  70 Batch    0/2   train_loss = 0.277\n",
      "Epoch  75 Batch    0/2   train_loss = 0.211\n",
      "Epoch  80 Batch    0/2   train_loss = 0.166\n",
      "Epoch  85 Batch    0/2   train_loss = 0.133\n",
      "Epoch  90 Batch    0/2   train_loss = 0.108\n",
      "Epoch  95 Batch    0/2   train_loss = 0.091\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating new text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Generation Functions\n",
    "### Get Tensors\n",
    "This function gets the tensors we need from the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    \n",
    "    InputTensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    FinalStateTensor  = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    \n",
    "    return (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "\n",
    "\n",
    "\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Next Word\n",
    "Function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "\n",
    "    # find the top 5 highest probability words, and pick one at random\n",
    "    pick_id = np.random.randint(1,2) # adjust randmoness as needed\n",
    "    highest_prob_index = 0\n",
    "    for i in range(pick_id):\n",
    "        highest_prob_index = np.argmax(probabilities)\n",
    "        probabilities[highest_prob_index] = 0\n",
    "        \n",
    "        \n",
    "    return int_to_vocab[highest_prob_index]\n",
    "\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "Finally! Let's see if the NN will write Nobel worthy prose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "\n",
      "alice: ’\n",
      "\n",
      "‘which is just the case with mine, ’ said the hatter.\n",
      "\n",
      "alice felt dreadfully puzzled. the hatter’s remark seemed to have no\n",
      "sort of meaning in it, and yet it was certainly english. ‘i don’t quite\n",
      "understand you, and listen to me! i’ll soon make you\n",
      "dry enough! ’ they all sat down at once, in a large ring, with the mouse\n",
      "in the middle. alice kept tossing\n",
      "the baby violently up and down, and the poor little thing howled so,\n",
      "that alice could hardly hear the words:-- ’\n",
      "\n",
      "she had quite forgotten the duchess by this time, and was a little\n",
      "startled when she heard her voice close to her ear. ‘you’re thinking\n",
      "about something, and\n",
      "perhaps after all it might tell her something worth hearing. for some\n",
      "minutes it puffed away without speaking, but at last it unfolded its\n",
      "arms, she\n",
      "kept fanning herself all the time she went on talking: ‘dear, dear! how\n",
      "queer everything upon bill! ’ said the hatter.\n",
      "\n",
      "alice felt dreadfully puzzled. the hatter’s remark seemed to have no\n",
      "sort of meaning in it, and yet it was certainly english. ‘i don’t quite\n",
      "understand you, and listen to me! i’ll soon make you\n",
      "dry enough! ’ they all sat down at once, in a large ring, with the mouse\n",
      "in the middle. alice kept tossing\n",
      "the baby violently up and down, and the poor little thing howled so,\n",
      "that alice could hardly hear the words:-- ’\n",
      "\n",
      "she had quite forgotten the duchess by this time, and was a little\n",
      "startled when she heard her voice close to her ear. ‘you’re thinking\n",
      "about something, and\n",
      "perhaps after all it might tell her something worth hearing. for some\n",
      "minutes it puffed away without speaking, but at last it unfolded its\n",
      "arms, she\n",
      "kept fanning herself all the time she went on talking: ‘dear, dear! how\n",
      "queer everything upon bill! ’ said the hatter.\n",
      "\n",
      "alice felt dreadfully puzzled. the hatter’s remark seemed to have no\n",
      "sort of meaning in it, and yet it was certainly english. ‘i don’t quite\n",
      "understand you, and listen to me! i’ll soon make you\n",
      "dry enough! ’ they all sat down at once, in a large ring, with the mouse\n",
      "in the middle. alice kept tossing\n",
      "the baby violently up and down, and the poor little thing howled so,\n",
      "that alice could hardly hear the words:-- ’\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_length = 500 # how many words to generate\n",
    "\n",
    "prime_word = 'alice' # the word to trigger it all.\n",
    "\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    init_text = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        init_text = init_text.replace(' ' + token.lower(), key)\n",
    "    init_text = init_text.replace('\\n ', '\\n')\n",
    "    init_text = init_text.replace('( ', '(')\n",
    "        \n",
    "    print()\n",
    "    print(init_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick random phrases from the generated text and check against the original text\n",
    "text.find('alice kept tossing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Little Absurd?\n",
    "Yeah, but so what. Have you seen the original? It kind of kept the spirit of it!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
